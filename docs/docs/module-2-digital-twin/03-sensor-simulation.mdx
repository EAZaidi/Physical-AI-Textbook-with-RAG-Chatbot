---
title: "Chapter 3: Sensor Simulation and Data Collection"
description: "Accurately simulate LiDAR, depth cameras, and IMUs for algorithm testing"
sidebar_position: 3
---

import CodeBlock from '@theme/CodeBlock';

# Chapter 3: Sensor Simulation and Data Collection

## 3.1 Introduction

Robotics algorithms—localization, mapping, object detection, navigation—depend on sensor data. Developing these algorithms on real hardware is expensive and time-consuming. Sensor simulation enables:

- **Rapid iteration**: Test perception algorithms with thousands of scenarios
- **Controlled experiments**: Add precise noise, vary lighting, create edge cases
- **Hardware-in-the-loop preparation**: Validate algorithms before deploying on real robots
- **Cost reduction**: Expensive sensors (Velodyne LiDAR: $8,000+) can be simulated for $0

This chapter covers three essential sensor types for humanoid robots:

1. **LiDAR (Light Detection and Ranging)**: 360° distance measurements for mapping and obstacle avoidance
2. **Depth Cameras**: RGB images with per-pixel depth for manipulation and visual navigation
3. **IMU (Inertial Measurement Unit)**: Acceleration and angular velocity for state estimation

We'll configure these sensors in Gazebo, collect synthetic data, and export it for algorithm development.

### Learning Objectives

By the end of this chapter, you will be able to:

1. Configure GPU-accelerated LiDAR sensors with realistic parameters
2. Set up depth cameras with calibration and noise models
3. Simulate IMU sensors with Gaussian noise characteristics
4. Collect synchronized sensor data using ROS 2
5. Export sensor data to standard formats (PCD point clouds, PNG depth images, CSV IMU logs)
6. Validate sensor accuracy against ground truth

---

## 3.2 LiDAR Sensor Configuration

LiDAR sensors emit laser pulses and measure time-of-flight to compute distances. Common robotics LiDARs:
- **2D**: Single scanning plane (e.g., SICK LMS511, $3,000)
- **3D**: Multi-layer or spinning sensors (e.g., Velodyne VLP-16, $8,000)

Gazebo supports both via the `gpu_ray` sensor type, which uses GPU for ray casting.

### LiDAR Configuration File

<CodeBlock language="xml" title="lidar_config.sdf" showLineNumbers>
{`<?xml version="1.0"?>
<sdf version="1.8">
  <model name="robot_with_lidar">
    <link name="base_link">
      <pose>0 0 0.5 0 0 0</pose>
      <!-- Basic collision and visual geometry -->
      <collision name="collision">
        <geometry>
          <box><size>0.3 0.3 0.5</size></box>
        </geometry>
      </collision>
      <visual name="visual">
        <geometry>
          <box><size>0.3 0.3 0.5</size></box>
        </geometry>
      </visual>

      <!-- LiDAR sensor -->
      <sensor name="lidar" type="gpu_ray">
        <pose>0 0 0.3 0 0 0</pose>  <!-- Mount 30cm above base -->
        <always_on>true</always_on>
        <update_rate>10</update_rate>  <!-- 10 Hz scan rate -->
        <visualize>true</visualize>  <!-- Show rays in GUI -->

        <ray>
          <scan>
            <horizontal>
              <samples>360</samples>  <!-- 360 rays = 1° resolution -->
              <resolution>1</resolution>
              <min_angle>0.0</min_angle>
              <max_angle>6.28318</max_angle>  <!-- 2π radians = 360° -->
            </horizontal>
          </scan>
          <range>
            <min>0.1</min>  <!-- Minimum detection distance (m) -->
            <max>30.0</max>  <!-- Maximum range (m) -->
            <resolution>0.01</resolution>  <!-- Distance resolution (cm) -->
          </range>
          <noise>
            <type>gaussian</type>
            <mean>0.0</mean>
            <stddev>0.01</stddev>  <!-- 1cm standard deviation -->
          </noise>
        </ray>

        <!-- ROS 2 plugin for publishing data -->
        <plugin name="lidar_plugin" filename="libgazebo_ros_ray_sensor.so">
          <ros>
            <namespace>/robot</namespace>
            <remapping>~/out:=lidar/scan</remapping>
          </ros>
          <output_type>sensor_msgs/LaserScan</output_type>
          <frame_name>lidar_link</frame_name>
        </plugin>
      </sensor>
    </link>
  </model>
</sdf>`}
</CodeBlock>

### Key Parameters

| Parameter | Value | Real-World Equivalent |
|-----------|-------|----------------------|
| `samples` | 360 | SICK LMS511: 360-1080 samples |
| `update_rate` | 10 Hz | Velodyne VLP-16: 5-20 Hz |
| `min_angle` / `max_angle` | 0 - 2π | 360° horizontal FOV |
| `min` / `max` range | 0.1 - 30m | VLP-16: 0.3 - 100m |
| `noise stddev` | 0.01m (1cm) | VLP-16 spec: ±3cm accuracy |

**Why `gpu_ray`?** CPU-based ray casting is slow for dense point clouds. GPU-accelerated sensors can simulate 100,000+ rays/second.

### Testing LiDAR

1. **Launch Gazebo with LiDAR model**:
```bash
gazebo lidar_config.sdf
```

2. **Visualize in RViz2** (separate terminal in Docker container):
```bash
ros2 run rviz2 rviz2
# Add -> By topic -> /robot/lidar/scan -> LaserScan
```

**Expected output**:
![RViz2 with LiDAR point cloud from Gazebo](./assets/screenshots/03-lidar-rviz.png)

Red dots show laser ray endpoints. Add obstacles (Insert → Box) and observe detection.

### Collecting LiDAR Data

Use ROS 2 to subscribe and process point clouds:

<CodeBlock language="python" title="collect_sensor_data.py" showLineNumbers>
{`#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import PointCloud2, LaserScan

class SensorDataCollector(Node):
    def __init__(self):
        super().__init__('sensor_data_collector')

        self.lidar_sub = self.create_subscription(
            LaserScan,
            '/robot/lidar/scan',
            self.lidar_callback,
            10
        )
        self.scan_count = 0

    def lidar_callback(self, msg):
        self.scan_count += 1
        if self.scan_count % 10 == 0:
            self.get_logger().info(
                f'LiDAR scan #{self.scan_count}: '\n                f'{len(msg.ranges)} points, '\n                f'range: {min(msg.ranges):.2f}-{max(msg.ranges):.2f}m'
            )

def main(args=None):
    rclpy.init(args=args)
    collector = SensorDataCollector()
    rclpy.spin(collector)
    collector.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()`}
</CodeBlock>

Run:
```bash
python3 collect_sensor_data.py
```

### Exporting Point Clouds

Convert LaserScan to 3D PointCloud2 and save to PCD format:

<CodeBlock language="python" title="export_pointcloud.py" showLineNumbers>
{`#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import PointCloud2
import sensor_msgs_py.point_cloud2 as pc2
import open3d as o3d
import numpy as np

class PointCloudExporter(Node):
    def __init__(self):
        super().__init__('pointcloud_exporter')

        self.subscription = self.create_subscription(
            PointCloud2,
            '/robot/lidar/points',
            self.pointcloud_callback,
            10
        )
        self.scan_count = 0
        self.max_scans = 100
        self.points_list = []

    def pointcloud_callback(self, msg):
        # Convert ROS PointCloud2 to numpy array
        points = pc2.read_points(msg, field_names=("x", "y", "z"), skip_nans=True)
        points_array = np.array([[p[0], p[1], p[2]] for p in points])

        self.points_list.extend(points_array)
        self.scan_count += 1

        if self.scan_count >= self.max_scans:
            self.export_pcd()
            raise SystemExit

    def export_pcd(self):
        self.get_logger().info(f'Exporting {len(self.points_list)} points...')

        # Create Open3D point cloud
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(np.array(self.points_list))

        # Save to file
        o3d.io.write_point_cloud('output.pcd', pcd)
        self.get_logger().info('Point cloud saved to output.pcd')

def main(args=None):
    rclpy.init(args=args)
    exporter = PointCloudExporter()
    try:
        rclpy.spin(exporter)
    except SystemExit:
        pass
    finally:
        exporter.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()`}
</CodeBlock>

**Output**: `output.pcd` file viewable in CloudCompare or MeshLab.

---

## 3.3 Depth Camera Configuration

Depth cameras (RGB-D sensors like Intel RealSense, Microsoft Kinect) provide color images + per-pixel depth.

### Depth Camera Configuration

<CodeBlock language="xml" title="depth_camera_config.sdf" showLineNumbers>
{`<?xml version="1.0"?>
<sdf version="1.8">
  <model name="robot_with_camera">
    <link name="base_link">
      <sensor name="depth_camera" type="depth_camera">
        <pose>0.15 0 0.1 0 0 0</pose>  <!-- Front-mounted -->
        <update_rate>30</update_rate>  <!-- 30 FPS -->
        <always_on>true</always_on>

        <camera>
          <horizontal_fov>1.047</horizontal_fov>  <!-- 60° FOV -->
          <image>
            <width>640</width>
            <height>480</height>
            <format>R8G8B8</format>  <!-- RGB -->
          </image>
          <clip>
            <near>0.5</near>  <!-- Minimum depth (m) -->
            <far>5.0</far>    <!-- Maximum depth (m) -->
          </clip>
          <noise>
            <type>gaussian</type>
            <mean>0.0</mean>
            <stddev>0.007</stddev>  <!-- 7mm depth noise -->
          </noise>
        </camera>

        <!-- ROS 2 plugin -->
        <plugin name="depth_camera_plugin" filename="libgazebo_ros_camera.so">
          <ros>
            <namespace>/robot</namespace>
            <remapping>~/image_raw:=camera/rgb/image_raw</remapping>
            <remapping>~/depth/image_raw:=camera/depth/image_raw</remapping>
            <remapping>~/camera_info:=camera/camera_info</remapping>
          </ros>
          <frame_name>camera_link</frame_name>
        </plugin>
      </sensor>
    </link>
  </model>
</sdf>`}
</CodeBlock>

### Key Parameters

| Parameter | Value | Real-World Equivalent |
|-----------|-------|----------------------|
| `horizontal_fov` | 60° (1.047 rad) | RealSense D435: 69° × 42° |
| `image` size | 640×480 | D435: up to 1280×720 |
| `clip` near/far | 0.5 - 5.0m | D435: 0.3 - 3.0m (indoor) |
| `noise stddev` | 0.007m (7mm) | D435 spec: &lt;2% at 2m |

### Visualizing Depth Images

In RViz2:
```bash
ros2 run rviz2 rviz2
# Add -> Image -> /robot/camera/rgb/image_raw (color)
# Add -> Image -> /robot/camera/depth/image_raw (depth)
```

![RViz2 RGB + depth images side-by-side](./assets/screenshots/03-depth-rviz.png)

Depth images show closer objects as darker (or colored based on distance).

### Exporting Depth Images

Save depth data as PNG:

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2

class DepthExporter(Node):
    def __init__(self):
        super().__init__('depth_exporter')
        self.bridge = CvBridge()

        self.subscription = self.create_subscription(
            Image,
            '/robot/camera/depth/image_raw',
            self.depth_callback,
            10
        )
        self.frame_count = 0

    def depth_callback(self, msg):
        # Convert ROS Image to OpenCV format
        depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')

        # Normalize for visualization (0-255)
        depth_normalized = cv2.normalize(depth_image, None, 0, 255, cv2.NORM_MINMAX)
        depth_uint8 = depth_normalized.astype('uint8')

        # Save
        cv2.imwrite(f'depth_frame_{self.frame_count:04d}.png', depth_uint8)
        self.frame_count += 1
```

---

## 3.4 IMU Sensor Configuration

IMUs measure linear acceleration (accelerometer) and angular velocity (gyroscope). Critical for:
- **State estimation**: Kalman filters fuse IMU + other sensors for pose estimation
- **Balance control**: Detect tilting and falling in humanoid robots
- **Dead reckoning**: Integrate acceleration for position estimates (short-term)

### IMU Configuration

<CodeBlock language="xml" title="imu_config.sdf" showLineNumbers>
{`<?xml version="1.0"?>
<sdf version="1.8">
  <model name="robot_with_imu">
    <link name="base_link">
      <!-- IMU sensor -->
      <sensor name="imu" type="imu">
        <pose>0 0 0 0 0 0</pose>  <!-- Torso center -->
        <always_on>true</always_on>
        <update_rate>100</update_rate>  <!-- 100 Hz (typical for IMU) -->

        <imu>
          <!-- Angular velocity (gyroscope) -->
          <angular_velocity>
            <x>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.001</stddev>  <!-- 0.001 rad/s noise -->
              </noise>
            </x>
            <y>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.001</stddev>
              </noise>
            </y>
            <z>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.001</stddev>
              </noise>
            </z>
          </angular_velocity>

          <!-- Linear acceleration (accelerometer) -->
          <linear_acceleration>
            <x>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.01</stddev>  <!-- 0.01 m/s² noise -->
              </noise>
            </x>
            <y>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.01</stddev>
              </noise>
            </y>
            <z>
              <noise type="gaussian">
                <mean>0.0</mean>
                <stddev>0.01</stddev>
              </noise>
            </z>
          </linear_acceleration>
        </imu>

        <!-- ROS 2 plugin -->
        <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">
          <ros>
            <namespace>/robot</namespace>
            <remapping>~/out:=imu/data</remapping>
          </ros>
          <frame_name>imu_link</frame_name>
        </plugin>
      </sensor>
    </link>
  </model>
</sdf>`}
</CodeBlock>

### Key Parameters

| Parameter | Value | Real-World Equivalent |
|-----------|-------|----------------------|
| `update_rate` | 100 Hz | Bosch BNO055: 100 Hz |
| Gyro noise | 0.001 rad/s | BNO055: 0.01°/s = 0.00017 rad/s |
| Accel noise | 0.01 m/s² | BNO055: ±0.01 m/s² (1mg) |

**Important**: IMU measures:
- **Acceleration**: Includes gravity! Stationary IMU reads `az = 9.81 m/s²` (not zero)
- **Angular velocity**: Rate of rotation, not orientation (requires integration)

### Collecting IMU Data

Export to CSV for analysis:

<CodeBlock language="python" title="export_imu_csv.py" showLineNumbers>
{`#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
import csv
import time

class IMUExporter(Node):
    def __init__(self):
        super().__init__('imu_exporter')

        self.subscription = self.create_subscription(
            Imu,
            '/robot/imu/data',
            self.imu_callback,
            10
        )

        # Open CSV file
        self.csv_file = open('imu_data.csv', 'w', newline='')
        self.csv_writer = csv.writer(self.csv_file)

        # Write header
        self.csv_writer.writerow([
            'timestamp', 'ax', 'ay', 'az', 'gx', 'gy', 'gz',
            'qx', 'qy', 'qz', 'qw'
        ])

        self.sample_count = 0
        self.max_samples = 1000
        self.start_time = None

    def imu_callback(self, msg):
        if self.start_time is None:
            self.start_time = time.time()

        timestamp = time.time() - self.start_time

        # Write to CSV
        self.csv_writer.writerow([
            f'{timestamp:.3f}',
            f'{msg.linear_acceleration.x:.6f}',
            f'{msg.linear_acceleration.y:.6f}',
            f'{msg.linear_acceleration.z:.6f}',
            f'{msg.angular_velocity.x:.6f}',
            f'{msg.angular_velocity.y:.6f}',
            f'{msg.angular_velocity.z:.6f}',
            f'{msg.orientation.x:.6f}',
            f'{msg.orientation.y:.6f}',
            f'{msg.orientation.z:.6f}',
            f'{msg.orientation.w:.6f}'
        ])

        self.sample_count += 1

        if self.sample_count >= self.max_samples:
            self.csv_file.close()
            self.get_logger().info('IMU data saved to imu_data.csv')
            raise SystemExit`}
</CodeBlock>

### Visualizing IMU Data

Plot acceleration and angular velocity:

<CodeBlock language="python" title="plot_imu_data.py" showLineNumbers>
{`#!/usr/bin/env python3
import pandas as pd
import matplotlib.pyplot as plt

def plot_imu_data(csv_file='imu_data.csv'):
    # Read CSV
    data = pd.read_csv(csv_file)

    # Create 2-panel plot
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

    # Plot linear acceleration
    ax1.plot(data['timestamp'], data['ax'], label='ax', linewidth=0.8)
    ax1.plot(data['timestamp'], data['ay'], label='ay', linewidth=0.8)
    ax1.plot(data['timestamp'], data['az'], label='az', linewidth=0.8)
    ax1.set_ylabel('Linear Acceleration (m/s²)')
    ax1.set_title('IMU Data Visualization')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Add gravity reference line
    az_mean = data['az'].mean()
    ax1.axhline(az_mean, color='green', linestyle='--',
                label=f'az mean: {az_mean:.2f} m/s²')

    # Plot angular velocity
    ax2.plot(data['timestamp'], data['gx'], label='gx', linewidth=0.8)
    ax2.plot(data['timestamp'], data['gy'], label='gy', linewidth=0.8)
    ax2.plot(data['timestamp'], data['gz'], label='gz', linewidth=0.8)
    ax2.set_xlabel('Time (s)')
    ax2.set_ylabel('Angular Velocity (rad/s)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('imu_plot.png', dpi=150)
    print('Plot saved to imu_plot.png')

if __name__ == '__main__':
    plot_imu_data()`}
</CodeBlock>

**Expected output**:
![IMU data plot showing acceleration and gyroscope readings](./assets/screenshots/03-imu-plot.png)

Stationary robot should show:
- `ax ≈ 0`, `ay ≈ 0`, `az ≈ 9.81 m/s²` (gravity)
- `gx ≈ 0`, `gy ≈ 0`, `gz ≈ 0` (no rotation)

Noise should be visible as small fluctuations around mean values.

---

## 3.5 Sensor Accuracy Validation

Simulation fidelity depends on accurate sensor models. Here's how to validate:

### LiDAR Accuracy Test

1. **Place known object** (e.g., 1m × 1m wall) at distance `d = 5.0m`
2. **Collect LiDAR scan** and measure detected distance `d_measured`
3. **Calculate error**: `error = |d - d_measured|`
4. **Acceptance**: Error < 5cm (typical LiDAR accuracy)

```python
# Extract range to wall
scan_msg = ...  # From /robot/lidar/scan
wall_index = len(scan_msg.ranges) // 2  # Front-facing ray
measured_distance = scan_msg.ranges[wall_index]

expected = 5.0  # meters
error = abs(expected - measured_distance)
print(f'Error: {error * 100:.2f} cm')  # Should be < 5cm
```

### Depth Camera Accuracy Test

Similar to LiDAR:
1. Place object at known depth
2. Sample central pixel from depth image
3. Compare to ground truth

**Noise characteristics**: Real depth cameras have:
- **Distance-dependent noise**: Noise increases with distance (2% of depth typical)
- **Material-dependent errors**: Reflective/transparent surfaces cause errors
- **Edge artifacts**: Depth discontinuities show artifacts

Gazebo's noise model is simplified (Gaussian), so add custom post-processing if needed.

### IMU Noise Validation

Check if noise matches configured standard deviation:

```python
# Compute standard deviation from stationary data
ax_std = data['ax'].std()
ay_std = data['ay'].std()
az_std = data['az'].std()

print(f'Accel noise: σx={ax_std:.4f}, σy={ay_std:.4f}, σz={az_std:.4f} m/s²')
# Should match configured stddev=0.01 m/s²
```

---

## 3.6 Exercises

### Exercise 1: LiDAR Mapping

**Objective**: Create a 2D occupancy grid map from LiDAR scans.

**Tasks**:
1. Create a Gazebo world with obstacles (boxes, cylinders)
2. Mount LiDAR on mobile base, drive through environment
3. Record LaserScan messages to rosbag: `ros2 bag record /robot/lidar/scan`
4. Use ROS 2 `slam_toolbox` to generate occupancy grid map
5. Visualize map in RViz2

**Deliverable**: Occupancy grid image (PNG), comparison to ground-truth world layout.

### Exercise 2: Depth-Based Object Detection

**Objective**: Detect objects using depth thresholding.

**Tasks**:
1. Place objects at varying depths (0.5m, 1.0m, 2.0m) in camera view
2. Collect depth images
3. Write Python script to:
   - Threshold depth (e.g., objects < 1.5m)
   - Apply connected component analysis (OpenCV `connectedComponents`)
   - Draw bounding boxes around detected objects
4. Compare to RGB image

**Deliverable**: Annotated image with bounding boxes, accuracy report (% objects detected).

### Exercise 3: IMU-Based Orientation Estimation

**Objective**: Integrate gyroscope data to estimate orientation.

**Tasks**:
1. Rotate robot in simulation (apply torque or use controller)
2. Collect IMU data (`angular_velocity`)
3. Integrate gyro readings: `θ(t) = θ(t-1) + ω(t) * dt`
4. Compare to ground-truth orientation from Gazebo (`/gazebo/model_states`)
5. Plot both on same graph

**Deliverable**: Plot showing estimated vs. true orientation, discussion of drift over time.

---

## Summary

In this chapter, you learned:

- ✅ How to configure GPU-accelerated LiDAR sensors with realistic noise models
- ✅ Depth camera setup for RGB-D data collection
- ✅ IMU simulation with accelerometer and gyroscope noise
- ✅ ROS 2 integration for sensor data streaming
- ✅ Exporting sensor data to standard formats (PCD, PNG, CSV)
- ✅ Validating sensor accuracy against ground truth

**Key Takeaways**:
1. **Sensor fidelity impacts algorithm performance**: Unrealistic noise models lead to brittle algorithms
2. **Synchronization matters**: Multi-sensor fusion requires time-aligned data
3. **Noise is necessary**: Algorithms trained on perfect data fail on real hardware
4. **Validate continuously**: Compare simulated sensor outputs to manufacturer datasheets

**Integration Point**: Combine Gazebo (Chapter 1) + Unity (Chapter 2) + Sensors (Chapter 3):
- Gazebo provides physics-accurate simulation
- Unity renders high-fidelity visuals
- Sensors bridge simulation to perception algorithms

In real-world workflows, you might:
1. Develop controllers in Gazebo (accurate physics)
2. Test vision algorithms in Unity (realistic rendering)
3. Validate on hardware with sensor data exported from simulation

---

## Additional Resources

- [Gazebo Sensors Documentation](https://gazebosim.org/docs/fortress/sensors)
- [ROS 2 sensor_msgs Package](https://github.com/ros2/common_interfaces/tree/rolling/sensor_msgs)
- [Open3D Point Cloud Library](http://www.open3d.org/docs/release/)
- [PCL (Point Cloud Library)](https://pointclouds.org/)
- [IMU Noise Models Paper](https://ieeexplore.ieee.org/document/6696917) - Allan variance analysis

**Estimated Time**: 4-5 hours to complete chapter and exercises
